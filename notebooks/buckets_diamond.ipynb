{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d0606c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n================================================================================\\nNombre del Script: cargar_pacientes_diamond.py\\nAutor: Félix Cárdenas\\nFecha de Creación: 2025-05-10\\nÚltima Modificación: 2025-05-10\\nVersión: 1.0.0\\n\\nDescripción:\\nEste script forma parte de la capa DIAMOND del proyecto BigData_Project.\\nSe encarga de tomar los datos refinados desde el bucket dev-silver en formato Parquet,\\nleerlos con Spark, inferir el schema y tipos de datos, y generar automáticamente\\nla tabla correspondiente en PostgreSQL si no existe. Luego realiza la carga de datos\\nmediante COPY desde un archivo CSV temporal. Finalmente, genera un backup en el bucket\\ndev-diamond con timestamp.\\n\\nPasos principales:\\n1. Lectura desde MinIO (SILVER) con boto3.\\n2. Procesamiento con Spark para obtención de schema.\\n3. Creación de tabla en PostgreSQL si no existe.\\n4. Inserción eficiente con COPY (psycopg2).\\n5. Backup del dataset insertado en MinIO (DIAMOND).\\n\\nDependencias:\\n- Python >= 3.8\\n- Librerías: pyspark, pandas, boto3, python-dotenv, psycopg2, logging\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Nombre del Script: cargar_pacientes_diamond.py\n",
    "Autor: Félix Cárdenas\n",
    "Fecha de Creación: 2025-05-10\n",
    "Última Modificación: 2025-05-10\n",
    "Versión: 1.0.0\n",
    "\n",
    "Descripción:\n",
    "Este script forma parte de la capa DIAMOND del proyecto BigData_Project.\n",
    "Se encarga de tomar los datos refinados desde el bucket dev-silver en formato Parquet,\n",
    "leerlos con Spark, inferir el schema y tipos de datos, y generar automáticamente\n",
    "la tabla correspondiente en PostgreSQL si no existe. Luego realiza la carga de datos\n",
    "mediante COPY desde un archivo CSV temporal. Finalmente, genera un backup en el bucket\n",
    "dev-diamond con timestamp.\n",
    "\n",
    "Pasos principales:\n",
    "1. Lectura desde MinIO (SILVER) con boto3.\n",
    "2. Procesamiento con Spark para obtención de schema.\n",
    "3. Creación de tabla en PostgreSQL si no existe.\n",
    "4. Inserción eficiente con COPY (psycopg2).\n",
    "5. Backup del dataset insertado en MinIO (DIAMOND).\n",
    "\n",
    "Dependencias:\n",
    "- Python >= 3.8\n",
    "- Librerías: pyspark, pandas, boto3, python-dotenv, psycopg2, logging\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07b8c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# PASO 1: IMPORTACIÓN DE LIBRERÍAS\n",
    "# ================================================================================\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05fbc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# PASO 2: CONFIGURACIÓN DE VARIABLES\n",
    "# ================================================================================\n",
    "\n",
    "load_dotenv(\"/home/jovyan/.env\")  # Ruta de tu .env\n",
    "# Logger\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Spark\n",
    "spark = SparkSession.builder.appName(\"Capa DIAMOND\").getOrCreate()\n",
    "\n",
    "#buckets \n",
    "BUCKET_SILVER     = os.getenv(\"MINIO_BUCKET_SILVER\")\n",
    "BUCKET_DIAMOND    = os.getenv(\"MINIO_BUCKET_DIAMOND\")\n",
    "#Coneccion a minio\n",
    "MINIO_ENDPOINT    = os.getenv(\"MINIO_ENDPOINT\")\n",
    "MINIO_ACCESS_KEY  = os.getenv(\"MINIO_ROOT_USER\")\n",
    "MINIO_SECRET_KEY  = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "#coneccion a base de datos\n",
    "POSTGRES_USER            = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD            = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "POSTGRES_DB            = os.getenv(\"POSTGRES_DB\")\n",
    "HOST_POSTGRES            = os.getenv(\"HOST_POSTGRES_NBK\")\n",
    "PORT_POSTGRES            = os.getenv(\"PORT_POSTGRES\") \n",
    "\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "dominio = \"pacientes\"\n",
    "ruta_parquet = f\"s3a://{BUCKET_SILVER}/LOCAL_{dominio.upper()}/pacientes_refinados_{today}*.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "063d7bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:35:47,065 - INFO - Archivo Parquet encontrado: s3://dev-silver/LOCAL_PACIENTES/pacientes_refinados_202505101923.parquet\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# PASO 3: DESCARGA DESDE MinIO CON BOTO3 Y LECTURA CON SPARK\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "# Configurar logging \n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "#Cliente Boto3 \n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "# Buscar el archivo parquet más reciente del dia\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "prefix = f\"LOCAL_{dominio.upper()}/pacientes_refinados_{today}\"\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=BUCKET_SILVER, Prefix=prefix)\n",
    "archivos = sorted(\n",
    "    [obj[\"Key\"] for obj in response.get(\"Contents\", []) if obj[\"Key\"].endswith(\".parquet\")],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "if not archivos:\n",
    "    raise FileNotFoundError(f\"No se encontró ningún archivo Parquet con prefijo: {prefix}\")\n",
    "\n",
    "key_silver = archivos[0]\n",
    "logging.info(f\"Archivo Parquet encontrado: s3://{BUCKET_SILVER}/{key_silver}\")\n",
    "\n",
    "#Descargar el archivo a /tmp\n",
    "ruta_local_parquet = f\"/tmp/{os.path.basename(key_silver)}\"\n",
    "with open(ruta_local_parquet, \"wb\") as f:\n",
    "    s3.download_fileobj(BUCKET_SILVER, key_silver, f)\n",
    "\n",
    "#Leer con Spark desde disco local \n",
    "df_diamond = spark.read.parquet(f\"file://{ruta_local_parquet}\")\n",
    "# Leer schema del archivo parquet ya cargado\n",
    "schema = df_diamond.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92fc3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# PASO 4: CREACIÓN DE DDL \n",
    "# ================================================================================\n",
    "# Mapeo directo de tipos Spark a PostgreSQL\n",
    "type_mapping = {\n",
    "    \"StringType\": \"TEXT\", \"IntegerType\": \"INTEGER\", \"LongType\": \"BIGINT\",\n",
    "    \"ShortType\": \"SMALLINT\", \"DoubleType\": \"DOUBLE PRECISION\", \"FloatType\": \"REAL\",\n",
    "    \"BooleanType\": \"BOOLEAN\", \"DateType\": \"DATE\", \"TimestampType\": \"TIMESTAMP\",\"DecimalType\": \"NUMERIC\"\n",
    "}\n",
    "\n",
    "# Construcción  de columnas\n",
    "columnas_sql = [\n",
    "    f\"{field.name} {type_mapping.get(type(field.dataType).__name__, 'TEXT')}\"\n",
    "    for field in schema.fields\n",
    "]\n",
    "\n",
    "archivo = os.path.basename(key_silver)  \n",
    "nombre_tabla = archivo.split(\"_\")[0].lower() + \"_diamond\"\n",
    "\n",
    "#nos conectamos a la base de datos\n",
    "conn = psycopg2.connect(\n",
    "    host=HOST_POSTGRES,\n",
    "    port=PORT_POSTGRES,\n",
    "    dbname=POSTGRES_DB,\n",
    "    user=POSTGRES_USER,\n",
    "    password=POSTGRES_PASSWORD\n",
    ")\n",
    "\n",
    "# Cursor ya conectado\n",
    "cur = conn.cursor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0ba4a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:35:47,219 - INFO - La tabla 'pacientes_diamond' ya existe. No se realizará la creación ni la inserción.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# PASO 5: CREACIÓN DE TABLA Y COPY EN POSTGRESQL\n",
    "# ================================================================================\n",
    "\n",
    "# Verificamos si la tabla ya existe\n",
    "cur.execute(f\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT FROM information_schema.tables \n",
    "        WHERE table_schema = 'public' AND table_name = %s\n",
    "    );\n",
    "\"\"\", (nombre_tabla,))\n",
    "existe_tabla = cur.fetchone()[0]\n",
    "\n",
    "if not existe_tabla:\n",
    "    # Crear la tabla si no existe\n",
    "    columnas_creacion = \",\\n    \".join(columnas_sql)\n",
    "    create_table_sql = f\"CREATE TABLE {nombre_tabla} (\\n    {columnas_creacion}\\n);\"\n",
    "    cur.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "    logging.info(f\"Tabla '{nombre_tabla}' creada correctamente.\")\n",
    "\n",
    "    # Guardar el DataFrame en CSV temporal para COPY\n",
    "    ruta_csv = f\"/tmp/{nombre_tabla}.csv\"\n",
    "    df_diamond.toPandas().to_csv(ruta_csv, index=False)\n",
    "\n",
    "    # Insertar con COPY\n",
    "    with open(ruta_csv, \"r\") as f:\n",
    "        cur.copy_expert(f\"COPY {nombre_tabla} FROM STDIN WITH CSV HEADER\", f)\n",
    "    conn.commit()\n",
    "    logging.info(f\"Datos insertados correctamente en la tabla '{nombre_tabla}'.\")\n",
    "else:\n",
    "    logging.info(f\"La tabla '{nombre_tabla}' ya existe. No se realizará la creación ni la inserción.\")\n",
    "\n",
    "# Cerrar conexión\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de8f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:35:50,183 - INFO - 🗂️ Backup subido a s3://dev-diamond/diamond/pacientes_diamond_202505102035.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# PASO 6 : RESPALDO EN MinIO - BUCKET DIAMOND\n",
    "# ================================================================================\n",
    "# Crear nombre y ruta para el backup\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "backup_key = f\"diamond/{nombre_tabla}_{timestamp}.csv\"\n",
    "\n",
    "buffer = BytesIO()\n",
    "df_diamond.toPandas().to_csv(buffer, index=False)\n",
    "buffer.seek(0)\n",
    "\n",
    "s3.upload_fileobj(buffer, BUCKET_DIAMOND, backup_key)\n",
    "\n",
    "logging.info(f\"Backup subido a s3://{BUCKET_DIAMOND}/{backup_key}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
