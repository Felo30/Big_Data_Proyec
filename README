# ğŸ“˜ Proyecto Big Data - Plataforma por Capas (Bronze, Silver, Diamond, Gold)

Este proyecto implementa un flujo completo de procesamiento de datos utilizando Apache Spark, MinIO y PostgreSQL organizados en capas: `bronze`, `silver`, `diamond` y `gold`.

---

## ğŸ”§ Servicios

Se utilizan los siguientes servicios dentro de contenedores Docker:

* ğŸ **Jupyter** (Python 3.11 + Spark)
* ğŸ˜ **PostgreSQL 15**
* ğŸª£ **MinIO** (almacenamiento de objetos compatible con S3)
* ğŸ”¥ **Spark** (modo master/worker)

---

## â–¶ï¸ CÃ³mo levantar y apagar el proyecto

### Levantar contenedores:

```bash
bash up.sh
```

### Detener contenedores:

```bash
bash down.sh
```

---

## ğŸ“‚ Estructura de Carpetas

```bash
infraestructura/          # Scripts de setup (buckets, mover archivos)
notebooks/                # ExploraciÃ³n y validaciÃ³n desde JupyterLab
pipelines/ingesta/        # Carga inicial a MinIO (bronze) y PostgreSQL (diamond)
servicios/jupyter/        # Dockerfile de Jupyter con Spark + JARs + librerÃ­as
transformacion/           # Transformaciones silver y diamond con Spark
```

---

## âš™ï¸ Consideraciones de Entorno y Kernel

* Todos los scripts `.py` se ejecutan **dentro del contenedor** `jupyter_bigdata`.
* El entorno de ejecuciÃ³n es **Python 3.11** con Spark 3.5 preconfigurado.
* Para correr scripts directamente:

```bash
docker exec -it jupyter_bigdata python /home/jovyan/pipelines/ingesta/guardar_csv_bronze.py
```

* Para notebooks `.ipynb`, acceder a Jupyter en:

```
http://localhost:8888
```

* El kernel correcto debe ser:

```
Python [conda env: base] *
```

* Las rutas internas estÃ¡n basadas en `/home/jovyan/`, por ejemplo:

```python
/home/jovyan/datos/csv/pacientes_crudo.csv
```

---

## âœ… Capas de Procesamiento

### ğŸŸ¤ Bronze

* Lectura de archivos CSV locales con Spark.
* Almacenamiento directo en MinIO (`s3a://dev-bronze/`) sin pasar por Pandas.
* Scripts:

  * `guardar_csv_bronze.py`

### âšª Silver

* Lectura de archivos de Bronze con Spark.
* Validaciones, limpieza y transformaciÃ³n de campos.
* Escritura en formato Parquet (`s3a://dev-silver/`).
* Scripts:

  * `transformar_datos_silver.py`

### ğŸ’ Diamond

* Lectura del archivo Parquet mÃ¡s reciente desde Silver.
* InserciÃ³n de datos a PostgreSQL (creaciÃ³n dinÃ¡mica de tabla si no existe).
* Backup del archivo transformado en MinIO (`s3a://dev-diamond/`).
* Scripts:

  * `refinar_datos_diamond.py`

### ğŸŸ¡ Gold

* (Opcional) PodÃ©s agregar tus consultas o transformaciones analÃ­ticas.

---

## ğŸ“¦ Buckets en MinIO

| Bucket        | PropÃ³sito                          |
| ------------- | ---------------------------------- |
| `dev-bronze`  | Archivos crudos CSV                |
| `dev-silver`  | Datos transformados Parquet        |
| `dev-diamond` | Respaldos + insumo para PostgreSQL |
| `dev-gold`    | Reportes, dashboards finales       |

---

## ğŸ“¥ PostgreSQL

* Cada ejecuciÃ³n crea una tabla con nombre derivado del dominio + `_diamond`.
  Por ejemplo: `pacientes_diamond`

* InserciÃ³n mediante COPY desde un archivo CSV temporal generado por Spark.

---

## ğŸ“Œ Notas Finales

* El archivo `.env` contiene todas las variables necesarias para MinIO, PostgreSQL, y rutas internas.
* Los scripts estÃ¡n modularizados por etapa: ingesta, transformaciÃ³n, backup.

---

ğŸ§  Proyecto mantenido por FÃ©lix CÃ¡rdenas - 2025
